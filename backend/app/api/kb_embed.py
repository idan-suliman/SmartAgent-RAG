# backend/app/api/kb_embed.py
"""
Knowledge Base Embedding API
----------------------------
Handles the generation of vector embeddings for the chunks:
1. Loads 'chunks.jsonl' generated by the indexer.
2. Identifies new chunks vs. chunks that can be reused (from previous runs).
3. Batches new chunks and sends them to the OpenAI Embedding API.
4. Saves the vectors to 'embeddings.npy' and metadata to 'embeddings_meta.json'.
"""

from __future__ import annotations

import json
import math
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
from fastapi import APIRouter, Header, HTTPException

from backend.app.settings import settings
from backend.app.core.utils import get_openai_client, JobStatusManager
from backend.app.core.security import require_admin

router = APIRouter(prefix="/kb", tags=["kb"])

# --- Constants ---
BATCH_SIZE = 60 

# =========================================================
# HELPER FUNCTIONS
# =========================================================

def _term_progress(prefix: str, done: int, total: int, started: float, extra: str = "") -> None:
    """Prints a progress bar to the terminal."""
    pct = (done / max(total, 1)) * 100.0
    elapsed = time.perf_counter() - started
    rate = done / max(elapsed, 1e-9)
    remaining = total - done
    eta = remaining / max(rate, 1e-9)
    sys.stdout.write(
        f"\r{prefix} {done}/{total} ({pct:5.1f}%) | {rate:6.2f}/sec | ETA {eta:6.1f}s {extra}   "
    )
    sys.stdout.flush()


def _load_chunks(chunks_path: Path) -> List[dict]:
    """Loads chunks from a JSONL file into a list."""
    rows: List[dict] = []
    with chunks_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                rows.append(json.loads(line))
    return rows


def _is_ctx_len_error(e: Exception) -> bool:
    """Detects if the error is due to OpenAI context length limits."""
    s = str(e)
    return ("maximum context length" in s) or ("requested" in s and "tokens" in s)


def _safe_truncate(text: str, max_chars: int) -> str:
    """Truncates text safely to avoid token limits."""
    if not isinstance(text, str):
        return ""
    t = text.strip()
    if not t:
        return " " 
    if len(t) <= max_chars:
        return t
    return t[:max_chars]


# =========================================================
# API ENDPOINTS
# =========================================================

@router.get("/embed/status")
def embed_status():
    """Returns the current status of the embedding job."""
    status_mgr = JobStatusManager(Path(settings.index_dir) / "status_embed.json")
    return status_mgr.load()


@router.post("/embed")
def build_embeddings(x_admin_code: str | None = Header(default=None)):
    """
    Starts the Embedding Job.
    Reads chunks.jsonl, computes vectors via OpenAI, saves to embeddings.npy.
    Supports resuming/incremental updates by reusing existing vectors by Chunk ID.
    """
    require_admin(x_admin_code)

    index_dir = Path(settings.index_dir)
    status_mgr = JobStatusManager(index_dir / "status_embed.json")
    
    # Check if a job is already running
    curr_status = status_mgr.load()
    if curr_status.get("state") == "running":
        raise HTTPException(status_code=409, detail="Embedding job already running")

    if not settings.openai_api_key:
        err = "OPENAI_API_KEY is missing in configuration."
        status_mgr.fail_job(err)
        raise HTTPException(status_code=400, detail=err)

    chunks_path = index_dir / "chunks.jsonl"
    if not chunks_path.exists():
        err = "chunks.jsonl not found. Run /kb/index first."
        status_mgr.fail_job(err)
        raise HTTPException(status_code=400, detail=err)

    rows = _load_chunks(chunks_path)
    if not rows:
        err = "chunks.jsonl is empty."
        status_mgr.fail_job(err)
        raise HTTPException(status_code=400, detail=err)

    # 1. Prepare Text & Apply Limits
    texts: List[str] = []
    trunc_count = 0
    trunc_total_chars = 0
    trunc_max_chars = 0
    trunc_examples: List[Dict[str, Any]] = []

    for i, r in enumerate(rows):
        t = (r.get("text", "") or "")
        orig_len = len(t)
        
        # Enforce character limit per chunk
        if orig_len > settings.embed_max_chars:
            cut = orig_len - settings.embed_max_chars
            trunc_count += 1
            trunc_total_chars += cut
            trunc_max_chars = max(trunc_max_chars, cut)

            if len(trunc_examples) < 5:
                trunc_examples.append({
                    "i": i,
                    "source_path": r.get("source_path"),
                    "title": r.get("title"),
                    "orig_chars": orig_len,
                    "kept_chars": settings.embed_max_chars,
                    "cut_chars": cut,
                })
        texts.append(_safe_truncate(t, settings.embed_max_chars))

    # --- INCREMENTAL LOGIC: Load Old Vectors ---
    old_chunks_path = index_dir / "chunks.old.jsonl"
    old_embed_path = index_dir / "embeddings.npy"
    id_to_vector_map = {} # chunk_id -> vector
    reused_count = 0

    if old_chunks_path.exists() and old_embed_path.exists():
        try:
            print("[EMBED] Loading old embeddings for reuse (ID-Based)...")
            old_rows = _load_chunks(old_chunks_path)
            old_vecs = np.load(old_embed_path)
            
            n_vecs = len(old_vecs)
            n_rows = len(old_rows)
            use_count = min(n_vecs, n_rows)
            
            if n_vecs != n_rows:
                 print(f"[EMBED][WARN] Mismatch: {n_rows} chunks vs {n_vecs} vectors. Reusing first {use_count} pairs.")
            
            for i in range(use_count):
                r = old_rows[i]
                v = old_vecs[i]
                cid = r.get("chunk_id")
                if cid:
                    id_to_vector_map[cid] = v

            print(f"[EMBED] Loaded {len(id_to_vector_map)} unique vectors for reuse.")

        except Exception as e:
            print(f"[EMBED][WARN] Failed to load old chunks: {e}")

    # 2. Identify New vs Old
    indices_to_embed = []
    final_vectors = [None] * len(texts)

    for i, row in enumerate(rows):
        cid = row.get("chunk_id")
        if cid and cid in id_to_vector_map:
            final_vectors[i] = id_to_vector_map[cid]
            reused_count += 1
        else:
            indices_to_embed.append(i)

    # 3. Initialize Status
    total_chunks = len(texts) 
    to_embed_count = len(indices_to_embed)
    total_batches = int(math.ceil(to_embed_count / BATCH_SIZE))
    started = time.perf_counter()

    base_status = {
        "total_chunks": total_chunks,
        "processed_chunks": reused_count,
        "total_batches": total_batches,
        "processed_batches": 0,
        "elapsed_sec": 0.0,
        "message": f"Embedding started. Reuse: {reused_count}, New: {to_embed_count}",
        "model": settings.embed_model,
        "provider": "openai",
        "trunc_count": trunc_count,
        "trunc_total_chars": trunc_total_chars,
        "phase": "embed"
    }
    status_mgr.start_job(base_status)

    last_status_update = time.perf_counter()
    status_every_sec = 0.75 

    ctx_fallback_total_tries = 0
    ctx_fallback_total_cut_chars = 0
    ctx_fallback_events: List[Dict[str, Any]] = []

    client = get_openai_client()

    try:
        # Processing Loop (Only NEW items)
        for b in range(total_batches):
            start_idx = b * BATCH_SIZE
            end_idx = start_idx + BATCH_SIZE
            batch_indices = indices_to_embed[start_idx:end_idx]
            
            batch_texts = [texts[ix] for ix in batch_indices]

            try:
                kwargs: Dict[str, Any] = {"model": settings.embed_model, "input": batch_texts}
                if settings.embed_dimensions:
                    kwargs["dimensions"] = settings.embed_dimensions

                resp = client.embeddings.create(**kwargs)
                new_vecs = [d.embedding for d in resp.data]
                
                # Assign to final vectors
                for local_i, vec in enumerate(new_vecs):
                    global_idx = batch_indices[local_i]
                    final_vectors[global_idx] = vec
                    
            except Exception as e:
                # Handle context length errors with retries/shrinking
                if _is_ctx_len_error(e):
                    print(f"\n[EMBED][CTX] Batch {b+1} failed context check. Switching to serial mode with retry.")
                    for local_k, txt_item in enumerate(batch_texts):
                        global_idx = batch_indices[local_k]
                        cur = txt_item
                        tries = 0
                        cut_total = 0
                        last_before_after = None
                        
                        while True:
                            try:
                                kwargs_single = {"model": settings.embed_model, "input": [cur]}
                                if settings.embed_dimensions:
                                    kwargs_single["dimensions"] = settings.embed_dimensions
                                
                                resp_single = client.embeddings.create(**kwargs_single)
                                new_v = resp_single.data[0].embedding
                                final_vectors[global_idx] = new_v
                                break
                            except Exception as inner_e:
                                if _is_ctx_len_error(inner_e) and len(cur) > 800 and tries < 6:
                                    before = len(cur)
                                    tries += 1
                                    cur = cur[: max(800, int(len(cur) * 0.75))]
                                    after = len(cur)
                                    cut = before - after
                                    cut_total += cut
                                    last_before_after = (before, after)
                                    print(f"[EMBED][CTX] Retry {tries}: shrinking {before}->{after}")
                                    continue
                                raise inner_e
                        
                        if tries > 0:
                            ctx_fallback_total_tries += tries
                            ctx_fallback_total_cut_chars += cut_total
                            if len(ctx_fallback_events) < 5:
                                ctx_fallback_events.append({
                                    "batch": b + 1,
                                    "tries": tries,
                                    "cut_total_chars": cut_total,
                                    "last_before_after": last_before_after,
                                })
                else:
                    raise e

            # Update Progress (UI)
            current_new_done = min((b + 1) * BATCH_SIZE, to_embed_count)
            processed_chunks = reused_count + current_new_done
            
            _term_progress("[EMBED]", processed_chunks, total_chunks, started)

            now = time.perf_counter()
            if (now - last_status_update >= status_every_sec) or (b == total_batches - 1):
                elapsed = now - started
                rate = processed_chunks / max(elapsed, 1e-9)
                rem = total_chunks - processed_chunks
                eta = rem / max(rate, 1e-9)

                base_status.update({
                    "processed_chunks": processed_chunks,
                    "processed_batches": b + 1,
                    "elapsed_sec": round(elapsed, 3),
                    "chunks_per_sec": round(rate, 3),
                    "eta_sec": round(eta, 1),
                    "message": f"Embedded batch {b+1}/{total_batches}",
                    "trunc_examples": trunc_examples, 
                    "ctx_fallback_events": ctx_fallback_events,
                })
                status_mgr.update(base_status)
                last_status_update = now

        # 4. Save Vectors
        if any(v is None for v in final_vectors):
            print("[EMBED][CRITICAL] Some vectors are None!")
        
        arr = np.array(final_vectors, dtype=np.float32)
        
        out_npy = index_dir / "embeddings.npy"
        out_meta = index_dir / "embeddings_meta.json"
        
        np.save(out_npy, arr)

        meta = {
            "count": int(arr.shape[0]),
            "dim": int(arr.shape[1]) if arr.ndim == 2 else None,
            "model": settings.embed_model,
            "provider": "openai",
            "output": str(out_npy),
            "updated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "trunc_count": trunc_count,
            "trunc_total_chars": trunc_total_chars,
            "reused_count": reused_count,
            "ctx_fallback_total_tries": ctx_fallback_total_tries,
            "ctx_fallback_total_cut_chars": ctx_fallback_total_cut_chars,
        }

        out_meta.write_text(
            json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
        )

        elapsed = time.perf_counter() - started
        base_status.update(meta)
        base_status.update({
            "elapsed_sec": round(elapsed, 3),
            "message": "Embedding completed"
        })
        status_mgr.complete_job(base_status)

        print()
        return {"ok": True, **meta}

    except Exception as e:
        status_mgr.fail_job(f"Embedding failed: {type(e).__name__}: {e}")
        raise